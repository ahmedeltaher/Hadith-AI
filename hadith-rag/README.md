# Arabic Hadith RAG Pipeline

A comprehensive Retrieval-Augmented Generation (RAG) system specialized for Arabic Hadith texts, built with LlamaIndex, Ollama, and Qdrant vector store.

## ğŸŒŸ Features

- **Arabic Hadith Processing**: Specialized pipeline for Arabic religious texts
- **Modern RAG Architecture**: LlamaIndex + Ollama + Qdrant integration
- **Smart Semantic Chunking**: Each Hadith becomes its own node based on semantic meaning boundaries
- **Arabic-Optimized Chunking**: Intelligent document segmentation that preserves Hadith integrity
- **Multiple Data Formats**: Support for .txt, .md, and .json files with automatic boundary detection
- **Interactive CLI**: Rich terminal interface for queries with semantic context
- **Modular Design**: Clean, extensible codebase with smart chunking algorithms

## ğŸ› ï¸ Technology Stack

- **LLM**: Ollama qwen2.5:7b (Chinese model with Arabic capabilities)
- **Embeddings**: Ollama qwen3-embedding:4b
- **Vector Store**: Qdrant (in-memory or server)
- **Framework**: LlamaIndex â‰¥0.11.0
- **Interface**: Typer + Rich for beautiful CLI
- **Chunking**: SemanticSplitterNodeParser for optimal Hadith boundaries

## ğŸš€ Quick Start

### Prerequisites

1. **Install Ollama** (https://ollama.ai)
2. **Pull required models**:
   ```bash
   ollama pull qwen2.5:7b
   ollama pull qwen3-embedding:4b
   ```

### Installation

#### Option 1: Automated Setup
```bash
git clone <repository-url>
cd hadith-rag
./setup.sh
```

#### Option 2: Conda Environment Setup
```bash
git clone <repository-url>
cd hadith-rag

# Create conda environment
conda env create -f environment.yml
conda activate hadith-rag

# Or use make commands
make conda-setup
conda activate hadith-rag
```

#### Option 3: Manual Setup
```bash
git clone <repository-url>
cd hadith-rag

# With pip
pip install -r requirements.txt

# Or with conda
conda create -n hadith-rag python=3.11 -y
conda activate hadith-rag
pip install -r requirements.txt
```

2. **Add your Hadith data** to the `data/` directory:
   - `.json` files with structure: `{"hadiths": [{"text": "...", ...}]}`
   - `.txt` or `.md` files with Hadith texts
   - Sample files included for testing

3. **Run the pipeline**:
   ```bash
   python main.py interactive
   ```

## ğŸ“– Usage

### Interactive Mode (with Semantic Chunking)

Start an interactive query session with smart semantic chunking:

```bash
python main.py interactive [OPTIONS]
```

Options:
- `--rebuild`: Rebuild the index from scratch
- `--no-semantic`: Disable semantic chunking (use simple chunking)
- `--top-k N`: Number of documents to retrieve (default: 5)
- `--data-dir PATH`: Custom data directory
- `--storage-dir PATH`: Custom storage directory

### Single Query

Execute a single query and exit:

```bash
python main.py query-single "Ù…Ø§ Ù‡Ùˆ Ø­Ø¯ÙŠØ« Ø§Ù„Ù†ÙŠØ©ØŸ"
```

### Build Index Only

Build or rebuild the document index with semantic chunking:

```bash
python main.py build-index [OPTIONS]
```

### System Check

Verify setup and dependencies:

```bash
python main.py check-setup
```

## ğŸ§  Semantic Chunking Features

### Smart Hadith Boundaries
- **Semantic Splitter**: Uses `SemanticSplitterNodeParser` to identify natural boundaries between Hadiths
- **Meaning Preservation**: Each Hadith becomes its own node based on semantic meaning rather than arbitrary character limits
- **Arabic Optimization**: 90% breakpoint threshold optimized for Arabic text patterns
- **Context Awareness**: Maintains relationships between related Hadiths while preserving individual integrity

### Benefits
- âœ… **Better Relevance**: Each node contains complete Hadith context
- âœ… **Improved Accuracy**: No Hadith text is split across multiple nodes
- âœ… **Semantic Understanding**: Boundaries determined by meaning, not length
- âœ… **Enhanced Retrieval**: More precise matching of user queries to relevant Hadiths

## ğŸ—‚ï¸ Project Structure

```
hadith-rag/
â”œâ”€â”€ main.py                 # CLI entry point with semantic chunking
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ environment.yml         # Conda environment configuration
â”œâ”€â”€ activate_env.sh         # Environment activation script
â”œâ”€â”€ data/                  # Hadith documents
â”‚   â”œâ”€â”€ sahih_bukhari_sample.json
â”‚   â””â”€â”€ hadith_collection.md
â”œâ”€â”€ storage/               # Vector index storage
â”œâ”€â”€ src/                   # Core modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py          # Configuration management
â”‚   â”œâ”€â”€ embeddings.py      # Custom Ollama embedding wrapper
â”‚   â”œâ”€â”€ document_loader.py # Multi-format document loading
â”‚   â”œâ”€â”€ index_builder.py   # Semantic chunking & vector index creation
â”‚   â””â”€â”€ query_engine.py    # Query processing & response generation
â””â”€â”€ README.md
```

## âš™ï¸ Configuration

Settings are managed in `src/config.py`:

```python
# Ollama settings
OLLAMA_BASE_URL = "http://localhost:11434"
EMBEDDING_MODEL = "qwen3-embedding:4b"
LLM_MODEL = "qwen2.5:7b"

# Retrieval settings
SIMILARITY_TOP_K = 5

# Semantic Chunking settings  
SEMANTIC_SPLITTER_BREAKPOINT_PERCENTILE_THRESHOLD = 90  # Optimized for Hadith boundaries
```

Environment variables can override defaults via `.env` file.

## ğŸ“Š Data Formats

### JSON Format

```json
{
  "collection": "sahih_bukhari",
  "hadiths": [
    {
      "text": "Arabic hadith text here...",
      "english": "English translation",
      "narrator": "Narrator name",
      "grade": "Sahih",
      "number": 1
    }
  ]
}
```

### Text/Markdown Format

```markdown
## Hadith Title

Arabic hadith text here...

## Another Hadith

More Arabic text...
```

The semantic chunking system automatically detects Hadith boundaries based on content structure and meaning.

## ğŸ¯ Example Queries

- "Ù…Ø§ Ù‡Ùˆ Ø­Ø¯ÙŠØ« Ø§Ù„Ù†ÙŠØ©ØŸ" (What is the hadith about intention?)
- "Ø£Ø­Ø§Ø¯ÙŠØ« Ø¹Ù† Ø¨Ø± Ø§Ù„ÙˆØ§Ù„Ø¯ÙŠÙ†" (Hadiths about honoring parents)
- "Ù‚Ø§Ù„ Ø±Ø³ÙˆÙ„ Ø§Ù„Ù„Ù‡ Ø¹Ù† Ø§Ù„ØµØ¯Ù‚" (What the Prophet said about truthfulness)
- "Ø£Ø­Ø§Ø¯ÙŠØ« ÙÙŠ ØµØ­ÙŠØ­ Ø§Ù„Ø¨Ø®Ø§Ø±ÙŠ Ø¹Ù† Ø§Ù„ØµÙ„Ø§Ø©" (Hadiths in Sahih Bukhari about prayer)

## ğŸ”§ Advanced Usage

### Programmatic Usage with Semantic Chunking

```python
from src import HadithQueryEngine, build_hadith_index

# Build index with semantic chunking (default)
index = build_hadith_index(
    data_dir="./data",
    use_sentence_window=False,  # Semantic chunking prioritized
    rebuild=True
)

# Create query engine  
engine = HadithQueryEngine(index, similarity_top_k=10)

# Query
result = engine.query("Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ØŸ")
print(result["answer"])
```

## ğŸ›¡ï¸ Best Practices

1. **Data Quality**: Ensure Arabic text is properly encoded (UTF-8)
2. **Semantic Chunking**: Default behavior provides optimal Hadith boundaries
3. **Index Management**: Use `--rebuild` when adding new documents
4. **Performance**: Semantic chunking may take slightly longer but provides better results
5. **Memory**: Qdrant in-memory mode suitable for smaller datasets

## ğŸš¨ Troubleshooting

### Common Issues

**Semantic Chunking Slow**:
```bash
# Check embedding model is working
python main.py check-setup

# Use simple chunking as fallback
python main.py interactive --no-semantic
```

**Ollama Connection Failed**:
```bash
# Check Ollama is running
ollama serve

# Verify models are available
ollama list
```

**Empty Index**:
```bash
# Rebuild with semantic chunking
python main.py build-index --rebuild
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make changes with proper documentation
4. Test semantic chunking with sample data
5. Submit a pull request

## ğŸ“œ License

This project is open source and available under the MIT License.

---

**Built with â¤ï¸ for Arabic Hadith preservation and accessibility**
**ğŸ§  Enhanced with smart semantic chunking for optimal Hadith boundaries**